{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rake\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from newsapi import NewsApiClient\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "import string\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import rake\n",
    "from gensim import models \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to put all questions from the same document in one string\n",
    "\n",
    "We already have a first good idea on how to organize the questions to extract the key words. At first, we have a csv file with the questions. We aggregate the questions that belong to the same document into an unique string. Then, we would extract the key words from this string.  \n",
    "\n",
    "In the preprocessing part, we still need, however, to tackle one basic thing and maybe a second one. The first is to lemmanize the dutch words. Here, we need to test the frog package. Second, we might need to improve (by including other words) the list of dutch stopwords. In the Github repository, I included the current list that we have. It's based on the NLTK package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Users/pieterzalis/Desktop/kamervragen_excel.xls')\n",
    "\n",
    "x = df['id'].unique() #select the unique documents\n",
    "df = df.set_index('id') #set the document number as index of the dataframe\n",
    "\n",
    "list_of_questions = [] # a list of string. Each string is the aggregation of all questions per document\n",
    "for i in x:\n",
    "    x = df.loc[i,'question'] #it localize all questions from the same document\n",
    "    total_questions = '' #this is initial string that aggregates all questions\n",
    "    for q in x:\n",
    "        total_questions += q\n",
    "    list_of_questions.append(total_questions)  \n",
    "\n",
    "x = df['topic'].unique() #here we extract the topic of the document.\n",
    "list_topics = []\n",
    "for i in x:\n",
    "    i = i.split('over') #The document has pattern: Some one questions over something. \n",
    "    i = i[1] #Here, we extract the part after the word dover.\n",
    "    list_topics.append(i)\n",
    "len(list_topics)\n",
    "\n",
    "final_list = [] \n",
    "for i in range(len(list_topics)): \n",
    "    \"\"\"loop to add related topics and questions\"\"\"\n",
    "    for w in range(len(list_of_questions)): \n",
    "        if i == w:\n",
    "            q = list_topics[i].replace('?', ' ') + ' ' + list_of_questions[w].replace('?', ' ')\n",
    "            final_list.append(q)\n",
    "total_key_words = []\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big challenge: What is the best way to extract key words?\n",
    "\n",
    "Rake didn't work out very well at first. After better lemmanizing the words and extracting the stopwords, it might work better. Additionally, I'll also prepare a code for two other alternatives: TF-IDF and LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Rake at first didn't work out very well at first. So now I'll try via TF-IDF gensim or LDA.\"\"\"\n",
    "\n",
    "\n",
    "##Rake code\n",
    "\n",
    "for i in final_list:\n",
    "    rake_object = rake.Rake(\"/Users/pieterzalis/RAKE-tutorial/data/stoplists/dutchstopwords.txt\", 3, 1, 1)\n",
    "    keywords = rake_object.run(i)\n",
    "    total_key_words.append(keywords)\n",
    "    \n",
    "\n",
    "#Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to query news articles based on the key words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from newsapi import NewsApiClient\n",
    "\n",
    "# newsapi = NewsApiClient(api_key='2b7935c2680f46b487d833129210d4c3')\n",
    "\n",
    "# for i in total_key_words:\n",
    "#     final_query = ''\n",
    "#     for w,q in i[:3]:\n",
    "#         print(w)\n",
    "#         query = w + ' AND '\n",
    "#         final_query += query\n",
    "#         final = final_query[:-5]\n",
    "#     all_articles = newsapi.get_everything(q=final,\n",
    "#                                       from_param='2018-19-10',\n",
    "#                                       to='2017-20-10',\n",
    "#                                       sort_by='relevancy',\n",
    "#                                       page=2)\n",
    "\n",
    "# # /v2/sources\n",
    "# all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
